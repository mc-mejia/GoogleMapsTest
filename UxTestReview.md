# Usability Testing Review

## Our Three Usability Tests

### First Test

We conducted our first usability test with a Williams college senior history major, E. We chose E as our first tester because she does not consider herself a "museum-person" and typically grows bored in a museum after about an hour. In addition, she does not own a smartwatch, and has never used one before and we wanted to see if our interface was clear for new users (as we expect many of our users to borrow a watch from the museum). We conducted our test in an empty grouping of carrels in the back part of sawyer library. We chose this space because we wanted to mimic the quiet of a museum gallery. Because this space was empty, we felt comfortable speaking aloud softly matching the volume we would use in a gallery. We hung up a small painting and presented the user with our watch and asked them to look at the painting and listen to other interpretations and offer their own in any order they would like. The watch prompted them to choose between listening and recording and the user was free to choose which one to engage in first. After they recorded their interpretation, the user was presented with a reward for offering an interpretation. When they listened to a recording, one member of our group served as the "wizard of oz" and read them a preprepared interpretation of the painting. In the second task, we asked them to click on the "thread" option in order to test the functionality of our collaborative feature. One member of our group again served as the wizard and read them a conversational list of interpretations. Then the user was prompted by the watch to offer their own interpretation. All three of our group members were present for the usability test. One group member served as the wizard that mimics the transition of pages depending on the person's actions, one member conducted the usability test, and one member served as the notetaker and human actor for the community thread.  The main issues that we found during this test were: 1) a lack of understanding with the star reward system, which we addressed with an informative pop-up, 2) the user hit the back button before submitting her recording, which we addressed with an error-prevention pop-up; and 3) swear words used in an interpretation, which we would address with an ai moniter.

### Second Test

For our second usability test, our user was a junior political science major, K. We used a procedure very similar to the first. The test took place in an empty room in Paresky, where we could have quiet and privacy, while still feeling free to speak aloud. We used the same painting and prepared interpretations as the first test, though we (the group members) changed roles (the wizard became the facilitator, the facilitator became the notetaker, and the notetaker became the wizard). After the first test, we chose to create a pre-recorded response thread for our Community Chat function, and we used that for this test. Another slight difference from the first test was that we chose to ask the participant to conduct each of our 2 main tasks directly, rather than let her decide freely. We did this because we wanted to ensure that we had a chance to show her each screen. In the course of our testing, we encountered a few moments of confusion (i.e. opportunities for revision). First, our user was confused when after submitting a recording, there was no feedback screen. The submit button was greyed-out again, but this feedback was not explicit or noticeable enough to signify that her response was submitted. Then, while listening to a community chat, our participant was confused by the forward and backward arrows. These arrows are meant to navigate between responses, but she thought they would change the screen. And finally, she did not fully understand the Star Reward screen. She thought it meant someone had rated her response highly, when it really means she’s been using InterprArt a lot or very well. Some of these issues could have been avoided had we revises our prototype between tests, but unfortunately we did not do this. To address these areas of confusion, we added labels to the arrows and used informative pop-ups with the recordign screen.

### Third Test

## Table of Results

## Two of Our Revisions, In More Detail

### First Revision

Clarifying the “next” and “previous” arrows on the Listening screens was a simple, but very important modification. Ambiguous icons can cause frustration and confusion. We saw this first hand with participant K as she attempted to leave the listening screen, but instead kept hearing a new interpretation (the exact opposite of what she wanted!). A frustration like this could cause someone to want to quit the app entirely, especially because the app is providing an elective service, not meeting a necessity. Improving the ease of interaction and reducing frustration increases the chances that users would actually use our app. This is a perfect example of how design the design process focuses on functionality rather than aesthetics. The simple arrows may look cleaner, but the labels increase usability.

## Overview Image of Our Final Prototype

## A Walkthrough Our Main Tasks

### Receive Rewards and/or Validation for Interpretations

### Facilitate Conversation About Art
